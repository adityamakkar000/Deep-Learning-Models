{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class MLP:\n",
    "    def __init__(self, learning_rate, *dims):\n",
    "        self.numLayers = len(dims) - 1  # numLayers - 1\n",
    "        self.layers = [i for i in range(self.numLayers + 1)]  # numLayers\n",
    "        self.dLayers = [i for i in range(self.numLayers)]  # (numLayers - 1)\n",
    "        self.layerWeights = [\n",
    "            np.random.randn(dims[i + 1], dims[i]) for i in range(0, self.numLayers)\n",
    "        ]  # (numLayers - 1)\n",
    "        self.layerBias = [\n",
    "            np.random.randn(dims[i], 1) for i in range(1, self.numLayers + 1)\n",
    "        ]  # (numLayers - 1)\n",
    "\n",
    "        self.lr = learning_rate\n",
    "\n",
    "    def get_total_params(self):\n",
    "        total = np.sum(\n",
    "            np.array([layer.shape[0] * layer.shape[1] for layer in self.layerWeights])\n",
    "        )\n",
    "        total += np.sum(np.array([layer.shape[0] for layer in self.layerBias]))\n",
    "        return total\n",
    "\n",
    "    def __call__(self, X):\n",
    "        def softmax(x):\n",
    "            return np.exp(x) / np.sum(np.exp(x), axis=1, keepdims=True)\n",
    "\n",
    "        self.layers[0] = X\n",
    "        for l in range(0, self.numLayers - 1):\n",
    "            self.layers[l + 1] = np.tanh(\n",
    "                self.layers[l] @ self.layerWeights[l].T + self.layerBias[l].T\n",
    "            )\n",
    "        self.layers[self.numLayers] = softmax(\n",
    "            self.layers[self.numLayers - 1] @ self.layerWeights[self.numLayers - 1].T\n",
    "            + self.layerBias[self.numLayers - 1].T\n",
    "        )\n",
    "\n",
    "        self.output = self.layers[self.numLayers]\n",
    "\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, dPrev):\n",
    "\n",
    "        batch_size = self.output.shape[0]\n",
    "        self.dW = [i for i in range(len(self.layerWeights))]\n",
    "        self.dB = [i for i in range(len(self.layerBias))]\n",
    "\n",
    "        \"\"\" Use this for self.dLayers if you have tanh at the end instead of softmax with MSE instead of CCE\"\"\"\n",
    "        # self.dOuput = (1/batch_size) * (y_true * (self.output ** -1)) # batch_size x ouput_dim\n",
    "        # self.dLayers[self.numLayers - 1] = self.dOuput * (1 - (self.output)**2) # batch_size x output_dim\n",
    "\n",
    "        self.dLayers[self.numLayers - 1] = dPrev\n",
    "\n",
    "        for l in range(self.numLayers - 2, -1, -1):\n",
    "            self.dLayers[l] = (self.dLayers[l + 1] @ self.layerWeights[l + 1]) * (\n",
    "                1 - (self.layers[l + 1]) ** 2\n",
    "            )\n",
    "\n",
    "        for l in range(0, self.numLayers):\n",
    "\n",
    "            B,T,C = self.dLayers[l].shape\n",
    "            print(self.dLayers[l].shape)\n",
    "            print(self.layers[l].shape)\n",
    "            self.dW[l] = (self.dLayers[l].reshape(B*T, -1).T) @ (self.layers[l].reshape(B*T, -1))\n",
    "            self.dB[l] = np.sum(self.dLayers[l], axis=0, keepdims=True)\n",
    "\n",
    "            self.layerWeights[l] -= self.lr * self.dW[l]\n",
    "            self.layerBias[l] -= self.lr * self.dB[l].T\n",
    "\n",
    "        self.dPrev = self.dLayer[0] @ self.layerWeights[0]\n",
    "\n",
    "        return self.dPrev\n",
    "\n",
    "\n",
    "class LayerNorm:\n",
    "\n",
    "    def __init__(self, learning_rate, last_dim):\n",
    "\n",
    "        self.lr = learning_rate\n",
    "        self.gamma = np.random.rand(1,1,last_dim)\n",
    "        self.beta = np.random.rand(1,1,last_dim)\n",
    "\n",
    "    def __call__(self, x):\n",
    "\n",
    "        self.mean = np.mean(x, axis=-1, keepdims=True)\n",
    "        self.std = np.std(x, axis=-1, keepdims=True)\n",
    "\n",
    "        self.x = x\n",
    "\n",
    "        x = (self.gamma / self.std) * (\n",
    "            x - self.mean\n",
    "        ) + self.beta  # broadcast across batch\n",
    "\n",
    "        return x\n",
    "\n",
    "    def backward(self, dPrev):\n",
    "\n",
    "        # prev -> (batch, dim)\n",
    "\n",
    "        dGamma = np.sum(dPrev * (self.x - self.mean) / self.std, axis=0)\n",
    "        dBeta = np.sum(dPrev, axis=0)\n",
    "        dX = self.gamma / self.std\n",
    "\n",
    "        self.beta -= self.lr * dBeta\n",
    "        self.gamma -= self.lr * dGamma\n",
    "\n",
    "        return dX\n",
    "\n",
    "\n",
    "class SelfAttention:\n",
    "\n",
    "    def __init__(self, learning_rate, dim, head_size):\n",
    "\n",
    "        self.lr = learning_rate\n",
    "        self.queries = MLP(self.lr, dim, head_size)\n",
    "        self.keys = MLP(self.lr, dim, head_size)\n",
    "        self.values = MLP(self.lr, dim, head_size)\n",
    "\n",
    "    def __call__(self, x):\n",
    "\n",
    "        def softmax(x):\n",
    "            return np.exp(x) / np.sum(np.exp(x), axis=1, keepdims=True)\n",
    "\n",
    "        q = self.queries(x)\n",
    "        k = self.keys(x)\n",
    "        v = self.values(x)\n",
    "\n",
    "\n",
    "        dotProduct = q @ k.transpose(0,2,1)\n",
    "        attention = softmax(np.tril(dotProduct) / q.shape[1])\n",
    "        logits = attention @ v\n",
    "\n",
    "        self.attention = attention\n",
    "        self.q = q\n",
    "        self.k = k\n",
    "        self.v = v\n",
    "\n",
    "        return logits\n",
    "\n",
    "    def backward(self, dPrev):\n",
    "\n",
    "        dLogits = dPrev # batch x l x channel\n",
    "\n",
    "        dAttention = dLogits @ self.v.T\n",
    "        dDotProduct = np.tril(\n",
    "            dAttention * (self.attention * (1 - self.attention)) / self.queries.shape[1]\n",
    "        )\n",
    "\n",
    "        dv = self.attention.T @ dLogits\n",
    "        dk = dDotProduct @ self.q\n",
    "        dq = dDotProduct @ self.k\n",
    "\n",
    "        self.dPrev = (\n",
    "            self.values.backward(dv)\n",
    "            + self.keys.backward(dk)\n",
    "            + self.queries.backward(dq)\n",
    "        )\n",
    "\n",
    "        return self.dPrev\n",
    "\n",
    "\n",
    "class MultiHeadSelfAttention:\n",
    "\n",
    "    def __init__(self, learning_rate, nheads, block_dim):\n",
    "\n",
    "        self.lr = learning_rate\n",
    "        head_size = block_dim // nheads\n",
    "        self.nheads = nheads\n",
    "        self.heads = [SelfAttention(self.lr, block_dim, head_size) for _ in range(nheads)]\n",
    "\n",
    "    def __call__(self, x):\n",
    "\n",
    "        logits = np.array([head(x) for head in self.heads])\n",
    "\n",
    "        logits = np.concatenate(logits, axis=-1)\n",
    "\n",
    "        return logits\n",
    "\n",
    "    def backward(self, dPrev):\n",
    "\n",
    "        dLogits = dPrev\n",
    "        dHeads = np.split(dLogits, indices_or_sections=self.nheads, axis=-1)\n",
    "        self.dPrev = np.sum(\n",
    "            np.array(\n",
    "                [\n",
    "                    self.heads[index].backward(dHeads[index])\n",
    "                    for index in range(self.nheads)\n",
    "                ]\n",
    "            ),\n",
    "            axis=0,\n",
    "        )\n",
    "\n",
    "        return self.dPrev\n",
    "\n",
    "\n",
    "class TransformerBlock:\n",
    "\n",
    "    def __init__(self, learning_rate, nheads, block_dim):\n",
    "\n",
    "        self.lr = learning_rate\n",
    "        self.MHA = MultiHeadSelfAttention(\n",
    "            self.lr, nheads=nheads, block_dim=block_dim\n",
    "        )\n",
    "        self.ln1 = LayerNorm(self.lr, block_dim)\n",
    "        self.ln2 = LayerNorm(self.lr, block_dim)\n",
    "        self.feedforward1 = MLP(self.lr, block_dim, 4 * block_dim, block_dim)\n",
    "        self.feedforward2 = MLP(self.lr, block_dim, 4 * block_dim, block_dim)\n",
    "\n",
    "    def __call__(self, x):\n",
    "\n",
    "        self.layer_norm_1 = self.ln1(x)\n",
    "        self.attention = self.MHA(self.layer_norm_1)\n",
    "        self.ff1 = self.feedforward1(self.attention)\n",
    "        self.inter = x + self.ff1\n",
    "\n",
    "        self.layer_norm_2 = self.ln2(self.inter)\n",
    "        self.ff2 = self.feedforward2(self.layer_norm_2)\n",
    "\n",
    "        logits = self.inter + self.ff2\n",
    "\n",
    "        return logits\n",
    "\n",
    "    def backward(self, dPrev):\n",
    "\n",
    "        dLogits = dPrev\n",
    "\n",
    "        dff2 = self.feedforward2.backward(dLogits)\n",
    "        dln2 = self.ln2.backward(dff2)\n",
    "\n",
    "        dInter = dLogits + dln2\n",
    "\n",
    "        dff1 = self.feedforward1.backward(dInter)\n",
    "        dAttention = self.MHA.backward(dff1)\n",
    "        dLn1 = self.ln1.backward(dAttention)\n",
    "\n",
    "        return dLn1\n",
    "\n",
    "\n",
    "class GPT:\n",
    "\n",
    "    def __init__(self, learning_rate, length, vocab_size, block_dim, nheads, nblocks):\n",
    "\n",
    "        self.lr = learning_rate\n",
    "\n",
    "        self.embedding_table = np.random.rand(vocab_size, block_dim)\n",
    "        self.pos_embedding = np.random.rand(length, block_dim)\n",
    "        self.blocks = [\n",
    "            TransformerBlock(self.lr, nheads, block_dim) for _ in range(nblocks)\n",
    "        ]\n",
    "        self.finalLayerNorm = LayerNorm(self.lr, block_dim)\n",
    "        self.mlp = MLP(self.lr, block_dim, vocab_size)\n",
    "\n",
    "    def __call__(self, x):\n",
    "\n",
    "        def softmax(arr):\n",
    "            return np.exp(arr) / np.sum(np.exp(arr), axis=1, keepdims=True)\n",
    "\n",
    "        self.indices = x\n",
    "        x = x @ self.embedding_table + self.pos_embedding\n",
    "\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "\n",
    "        logits = self.mlp(self.finalLayerNorm(x))\n",
    "\n",
    "        self.predictions = softmax(logits)\n",
    "\n",
    "        return self.predictions\n",
    "\n",
    "    def backward(self, y_true, train=True):\n",
    "\n",
    "        def CCE(predictions, y_true):\n",
    "            # assuming y_true is in the form of 1 hot embeddings\n",
    "            loss = -1 * np.sum(y_true * np.log(predictions))\n",
    "            return loss\n",
    "\n",
    "        def MSE(predictions, y_true):\n",
    "            loss = (\n",
    "                0.5\n",
    "                * (1 / predictions.shape[0])\n",
    "                * np.sum(np.sum((predictions - y_true) ** 2, axis=-1), axis=0)\n",
    "            )\n",
    "            return loss\n",
    "\n",
    "        loss = CCE(self.predictions, y_true)  # scalar 1,\n",
    "\n",
    "        if train == False:\n",
    "          return loss\n",
    "\n",
    "        dLoss = self.predictions - y_true\n",
    "\n",
    "\n",
    "        dFinalMlp = self.mlp.backward(dLoss)\n",
    "        dFinalLayerNorm = self.finalLayerNorm.backward(dFinalMlp)\n",
    "\n",
    "        dBlock = dFinalLayerNorm\n",
    "\n",
    "        for block in reversed(self.blocks):\n",
    "            dBlock = block.backward(dBlock)\n",
    "\n",
    "        dEmbedded = self.indices.T @ dBlock\n",
    "        self.embedding_table -= self.lr * dEmbedded\n",
    "        self.pos_embedding -= self.lr* dBlock\n",
    "\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s]\n",
    "decode = lambda l: ''.join([itos[i] for i in l])\n",
    "\n",
    "data = np.array(encode(text))\n",
    "\n",
    "n = int(0.9*len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "block_size = 8\n",
    "train_data[:block_size+1]\n",
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size+1]\n",
    "batch_size=16\n",
    "block_size=32\n",
    "\n",
    "def one_hot(array):\n",
    "    length = array.size\n",
    "    ans = np.zeros((length, vocab_size))\n",
    "    ans[np.arange(length), array] = 1\n",
    "    return ans\n",
    "\n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = np.random.randint(0,len(data) - block_size, (batch_size,))\n",
    "    x = np.stack([one_hot(data[i:i+block_size]) for i in ix])\n",
    "    y = np.stack([one_hot(data[i+1:i+block_size+1]) for i in ix])\n",
    "    return x, y\n",
    "\n",
    "x, y = get_batch('train')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 32, 65)\n",
      "(16, 32, 256)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (65,1) (65,32,1) (65,1) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[177], line 21\u001b[0m\n\u001b[1;32m     19\u001b[0m B,T,C \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m     20\u001b[0m probs \u001b[38;5;241m=\u001b[39m gpt(x)\n\u001b[0;32m---> 21\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mgpt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[176], line 294\u001b[0m, in \u001b[0;36mGPT.backward\u001b[0;34m(self, y_true, train)\u001b[0m\n\u001b[1;32m    289\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m loss\n\u001b[1;32m    291\u001b[0m dLoss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictions \u001b[38;5;241m-\u001b[39m y_true\n\u001b[0;32m--> 294\u001b[0m dFinalMlp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmlp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdLoss\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    295\u001b[0m dFinalLayerNorm \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinalLayerNorm\u001b[38;5;241m.\u001b[39mbackward(dFinalMlp)\n\u001b[1;32m    297\u001b[0m dBlock \u001b[38;5;241m=\u001b[39m dFinalLayerNorm\n",
      "Cell \u001b[0;32mIn[176], line 69\u001b[0m, in \u001b[0;36mMLP.backward\u001b[0;34m(self, dPrev)\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdB[l] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdLayers[l], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayerWeights[l] \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlr \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdW[l]\n\u001b[0;32m---> 69\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayerBias\u001b[49m\u001b[43m[\u001b[49m\u001b[43ml\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdB\u001b[49m\u001b[43m[\u001b[49m\u001b[43ml\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdPrev \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdLayer[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m@\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayerWeights[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdPrev\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (65,1) (65,32,1) (65,1) "
     ]
    }
   ],
   "source": [
    "max_iters = 20\n",
    "block_dim = 4 * 64\n",
    "\n",
    "\n",
    "params = {\n",
    "  \"learning_rate\": 0.01,\n",
    "  \"length\": block_size,\n",
    "  \"vocab_size\": vocab_size,\n",
    "  \"block_dim\": block_dim,\n",
    "  \"nheads\": 4,\n",
    "  \"nblocks\": 4\n",
    "\n",
    "          }\n",
    "\n",
    "gpt = GPT(**params)\n",
    "\n",
    "for iter in range(5000):\n",
    "  x,y = get_batch('train')\n",
    "  B,T,C = x.shape\n",
    "  probs = gpt(x)\n",
    "  loss = gpt.backward(y, True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
